{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":56537,"databundleVersionId":8015876,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T15:29:42.841697Z","iopub.execute_input":"2024-05-04T15:29:42.842604Z","iopub.status.idle":"2024-05-04T15:29:43.900900Z","shell.execute_reply.started":"2024-05-04T15:29:42.842571Z","shell.execute_reply":"2024-05-04T15:29:43.899466Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/leap-atmospheric-physics-ai-climsim/sample_submission.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:29:43.903602Z","iopub.execute_input":"2024-05-04T15:29:43.904267Z","iopub.status.idle":"2024-05-04T15:29:50.324773Z","shell.execute_reply.started":"2024-05-04T15:29:43.904222Z","shell.execute_reply":"2024-05-04T15:29:50.324022Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 读取数据\ntrain_csv = '/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv'\ntest_csv = '/kaggle/input/leap-atmospheric-physics-ai-climsim/test.csv'\nsubm_spl = '/kaggle/input/leap-atmospheric-physics-ai-climsim/sample_submission.csv'\nout_csv = 'submission.csv'\n\nread_chunk_size = 100000 # 一次性读取100000行数据\n\n# 训练参数\nnum_epochs = 50\nmax_patience = 3\nbatch_size = 360\nnum_workers = 256\nlr = 0.0005\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:29:50.325852Z","iopub.execute_input":"2024-05-04T15:29:50.326231Z","iopub.status.idle":"2024-05-04T15:29:50.358893Z","shell.execute_reply.started":"2024-05-04T15:29:50.326207Z","shell.execute_reply":"2024-05-04T15:29:50.357939Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def graph(acc, loss, title):\n    \"\"\" 绘制准确率和损失曲线\n\n    Args:\n        acc (list): [train, val]/[test]\n        loss (list): [train, val]/[test]\n        title (str): Title\n    \"\"\"\n    assert len(acc) == len(loss), 'Length of acc and loss must be the same'\n    global save_run_dir\n    if len(acc) == 2:\n        plt.subplot(1, 2, 1)\n        plt.plot(acc[0], label='Training Accuracy')\n        plt.plot(acc[1], label='Validation Accuracy')\n        plt.title(title)\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend()\n        plt.subplot(1, 2, 2)\n        plt.plot(loss[0], label='Training Loss')\n        plt.plot(loss[1], label='Validation Loss')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.legend()\n    elif len(acc) == 1:\n        plt.subplot(2, 1, 1)\n        plt.plot(acc[0], label='Test Accuracy')\n        plt.title(title)\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend()\n        plt.subplot(2, 1, 2)\n        plt.plot(loss[0], label='Test Loss')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:29:50.361666Z","iopub.execute_input":"2024-05-04T15:29:50.362297Z","iopub.status.idle":"2024-05-04T15:29:50.372223Z","shell.execute_reply.started":"2024-05-04T15:29:50.362272Z","shell.execute_reply":"2024-05-04T15:29:50.371513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import time\n\nclass Timer:\n    def __init__(self):\n        self.t0 = 0\n        self.t1 = 0\n        self.times = []\n        self.infos = []\n        \n    def start(self, info = 'Run'):\n        self.t1 = self.t0\n        self.infos.append(f'{len(self.infos)} {info}')\n        self.t0 = time.time()\n        \n    def stop(self):\n        self.t1 = time.time()\n        t = self.t1 - self.t0\n        self.times.append(t)\n        print(f'{self.infos[-1]} Time Cost: {t:.3f}s')\n        \n    def get_stats(self):\n        for info, tm in zip(self.infos, self.times):\n            print(f'{info}\\t{tm:.3f}s')\n        print(f'Total: {sum(self.times):.3f} ')\n    \n    def clear(self, idx=0):\n        if idx == 0:\n            self.infos.clear()\n            self.times.clear()\n            return\n        info = self.infos.pop(idx - (0 if idx < 0 else 1))\n        tm = self.times.pop(idx - (0 if idx < 0 else 1))\n        return info, tm\n        \ntimer = Timer()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:29:50.373317Z","iopub.execute_input":"2024-05-04T15:29:50.373607Z","iopub.status.idle":"2024-05-04T15:29:50.386515Z","shell.execute_reply.started":"2024-05-04T15:29:50.373583Z","shell.execute_reply":"2024-05-04T15:29:50.385601Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 读取数据\ntimer.start(f'Read dataset chunk size {read_chunk_size}')\ntrain_chunks = pd.read_csv(train_csv, chunksize = read_chunk_size)\ntrain_data = next(train_chunks)\ntimer.stop()\n# train_data = next(train_chunks)\n# train_data = next(train_chunks)\ncols = train_data.columns\n\ntrain_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:29:50.387435Z","iopub.execute_input":"2024-05-04T15:29:50.387684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 划分数据集\n\n1. 划分数据的输入输出\n2. 划分 训练集，验证集(，测试集)","metadata":{}},{"cell_type":"code","source":"# 划分数据集\n\ndef split_io(dframe, in_cols = cols[1:557], out_cols = cols[557:]):\n    # 划分输入输出\n    in_df = dframe[in_cols]\n    out_df = dframe[out_cols]\n    return in_df, out_df\n\ndef split_tvt(dframe, ratio=[0.8], shuffle=False):\n    # 设定好 train (和 val) 集的比例，剩余的均归到test/val\n    assert sum(ratio) <= 1, \"Ratio sum for train and val cannot be bigger than 1\"\n    assert len(ratio) > 0, \"Ratio cannot be empty\"\n     # 根据是否需要测试集来调整比例\n    train_rat = ratio[0]\n    val_rat = (1 - train_rat) if len(ratio) == 1 else ratio[1]\n    test_rat = (1 - train_rat - val_rat) if len(ratio) == 2 else 0\n    \n    data_size = dframe.shape[0]\n    train_size = int(data_size * train_rat)\n    val_size = (data_size - train_size) if len(ratio) == 1 else int(data_size * val_rat)\n    \n    if shuffle:\n        dframe = dframe.sample(frac=1).reset_index(drop=True)\n        \n    train = dframe.iloc[:train_size]\n    val = dframe.iloc[train_size:train_size + val_size]\n    \n    if test_rat != 0:\n        test = dframe.iloc[train_size + val_size:]\n        return train, val, test\n    else:\n        return train, val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 划分数据集\ntrain_set, val_set, test_set = split_tvt(train_data, [0.7, 0.2], True)\ntrain_in, train_out = split_io(train_set)\nval_in, val_out = split_io(val_set)\ntest_in, test_out = split_io(test_set)\n\nprint(f'{val_set.shape = }')\nprint(f'{test_set.shape = }')\nprint(f'{train_in.shape = }')\nprint(f'{train_out.shape = }')\nprint(f'{val_in.shape = }')\nprint(f'{val_out.shape = }')\n# print(f'{val_out.iloc[0] = }')\n\ndel(train_data)\ndel(train_set)\ndel(val_set)\ndel(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP_Dataset(Dataset):\n    def __init__(self, dsin, dsout, transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])):\n        self.dsin = dsin\n        self.dsout = dsout\n        self.transform =transform\n    \n    def __len__(self):\n        return self.dsin.shape[0]\n    \n    def __getitem__(self, idx):\n        data, targ =  self.dsin.iloc[idx], self.dsout.iloc[idx]\n        data, targ = data.to_numpy().reshape((1, 556)), targ.to_numpy((1, 368))\n        if self.transform != None:\n            data, targ = self.transform(data), self.transform(targ)\n        data, targ = data.astype(torch.float32), targ.astype(torch.float32)\n        return data, targ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MLP_Dataset(train_in, train_out)\n# print(f'{train_dataset[0][0].shape = } {train_dataset[0][1].shape = } {len(train_dataset) = }')\ntrain_loader =  DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nprint(f'{len(train_loader) = }')\n\nval_dataset = MLP_Dataset(val_in, val_out)\nval_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\nprint(f'{len(val_loader) = }')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_chunk_loader(chunk, test = False):\n    tmr = Timer()\n    tmr.start('Load datasets...')\n    train_set, val_set = split_tvt(chunk)\n    train_in, train_out = split_io(train_set)\n    val_in, val_out = split_io(val_set)\n    print(f'{train_in.shape = }')\n    print(f'{train_out.shape = }')\n    print(f'{val_in.shape = }')\n    print(f'{val_out.shape = }')\n    train_dataset = MLP_Dataset(train_in, train_out)\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    print(f'{len(train_dataset) = }')\n\n    val_dataset = MLP_Dataset(val_in, val_out)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    print(f'{len(val_dataset) = }')\n    tmr.stop()\n    return train_loader, val_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Layer(nn.Module):\n    def __init__(self, hidden):\n        super(Layer, self).__init__()\n        self.layer = nn.Sequential(\n            nn.LazyLinear(hidden),\n            nn.LazyBatchNorm1d(),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n    def forward(self, x):\n        x = self.layer(x)\n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, layers=5, hidden=144):\n        super(MLP, self).__init__()\n        self.hidden = hidden\n        self.linear = nn.Sequential(\n            *[Layer(hidden),] * (layers-1),\n            nn.LazyLinear(368),\n        )\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = MLP()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\n    \noptimizer = optim.Adam(net.parameters(), lr=lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lowest_loss = float('inf')\n\naccus, losses = [[],[]], [[],[]]\nepoch = 0\nlast_epoch = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = net.to(device)\nfor idx, chunk in enumerate(train_chunks):\n    patience = 0\n    timer.start(f'Train {num_epochs} epochs on chunk {idx}')\n    train_loader, val_loader = get_chunk_loader(chunk)\n    while epoch < num_epochs:\n        epoch += 1\n        t0 = time.time()\n        net.train()\n        train_loss = 0.0\n        train_accu = 0.0\n        num_train_batches = 0\n\n        for inp, outp in tqdm(train_loader):\n#             inp, outp = [inps.to(device, non_blocking=True) for inps in inp], outp.to(device)\n            inp, outp = inp.to(device, non_blocking = True), outp.to(device, non_blocking = True)\n            out_h = net(inp)\n            crit = criterion(out_h, outp)\n            loss = crit.item()\n            train_loss += loss\n            accu = r2_score(out_h, outp)\n            train_accu += accu\n            crit.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            num_train_batches += 1\n        avg_train_loss = train_loss / num_train_batches\n        losses[0].append(avg_train_loss)\n        avg_train_accu = train_accu / num_train_batches\n        accus[0].append(avg_train_accu)\n\n        net.eval()\n        val_loss = 0.0\n        val_accu = 0.0\n        num_val_batches = 0\n\n        with torch.no_grad():\n            for inp, outp in tqdm(val_loader):\n#                 inp, outp = [inps.to(device, non_blocking=True) for inps in inp], outp.cuda()\n                inp, outp = inp.to(device, non_blocking = True), outp.to(device, non_blocking = True)\n                out_h = net(inp)\n                crit = criterion(out_h, outp)\n                val_loss += crit.item()\n                accu = r2_score(out_h, outp)\n                val_accu += accu\n                num_val_batches += 1\n        try:\n            last_val_loss = avg_val_loss\n        except:\n            last_val_loss = lowest_loss\n        avg_val_loss = val_loss / num_val_batches\n        losses[1].append(avg_val_loss)\n        avg_val_accu = val_accu / num_val_batches\n        accus[1].append(avg_val_accu)\n\n        if avg_val_loss < lowest_loss:\n            torch.save(net.state_dict(), 'best.pth')  # 保存模型参数而不是整个模型\n            lowest_loss = avg_val_loss\n\n        t1 = time.time()\n        print(f'Chunk {idx} | Epoch {epoch - last_epoch}/{epoch} > Time Cost: {t1-t0:.2f}s | patience: {patience} \\n\\t', \n              f'Train Loss: {avg_train_loss:.3f} | Val Loss: {avg_val_loss:.3f}\\n\\t',\n              f'Train Accu: {avg_train_accu:.3f} | Val Accu: {avg_val_accu:.3f}')\n        if avg_train_loss < avg_val_loss and last_val_loss < avg_val_loss:\n            patience += 1\n#         else:\n#             patience = 0\n        if patience >= max_patience:\n            print(f'{max_patience} epochs had val loss bigger than train loss. Exit for next chunk of data')\n            patience = 0\n            last_epoch = epoch\n            del(chunk)\n            break\n    timer.stop()\n    torch.save(net.state_dict(), \"latest.pth\") \ngraph(accus, losses, 'Train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}